{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 課題１"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, glob, time, copy, random, zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check Current Directory\n",
    "os.listdir('./dogs-vs-cats-reduced')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train_dir, Test_dir\n",
    "base_dir = './dogs-vs-cats-reduced'\n",
    "train_dir = base_dir + '/train'\n",
    "test_dir  = base_dir + '/test'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check File Name\n",
    "os.listdir(train_dir)[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FilePath List\n",
    "train_list = glob.glob(os.path.join(train_dir , '*.jpg'))\n",
    "test_list = glob.glob(os.path.join(test_dir, '*.jpg'))\n",
    "\n",
    "train_list = [string.replace('\\\\', '/') for string in train_list]\n",
    "test_list = [string.replace('\\\\', '/') for string in test_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = Image.open(train_list[0])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = Image.open(test_list[0])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Label is contained in filepath\n",
    "train_list[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Image_Id is contained in filepath\n",
    "test_list[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get Label\n",
    "train_list[0].split('/')[-1].split('.')[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get Image_Id\n",
    "int(test_list[0].split('/')[-1].split('.')[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of Train Image\n",
    "len(train_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Divide Train, Valid Data\n",
    "train_list, val_list = train_test_split(train_list, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(train_list))\n",
    "print(len(val_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Augumentation\n",
    "class ImageTransform():\n",
    "    \n",
    "    def __init__(self, resize, mean, std):\n",
    "        self.data_transform = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(resize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "    def __call__(self, img, phase):\n",
    "        return self.data_transform[phase](img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class DogvsCatDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, file_list, transform=None, phase='train'):    \n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        img_transformed = self.transform(img, self.phase)\n",
    "        \n",
    "        # Get Label\n",
    "        label = img_path.split('/')[-1].split('.')[0]\n",
    "        if label == 'dog':\n",
    "            label = 1\n",
    "        elif label == 'cat':\n",
    "            label = 0\n",
    "\n",
    "        return img_transformed, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Config\n",
    "size = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_dataset = DogvsCatDataset(train_list, transform=ImageTransform(size, mean, std), phase='train')\n",
    "val_dataset = DogvsCatDataset(val_list, transform=ImageTransform(size, mean, std), phase='val')\n",
    "\n",
    "# Operation Check\n",
    "print('Operation Check')\n",
    "index = 0\n",
    "print(train_dataset.__getitem__(index)[0].size())\n",
    "print(train_dataset.__getitem__(index)[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloader_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "# Operation Check\n",
    "print('Operation Check')\n",
    "batch_iterator = iter(train_dataloader)\n",
    "inputs, label = next(batch_iterator)\n",
    "print(inputs.size())\n",
    "print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# VGG16 Model Loading\n",
    "use_pretrained = True\n",
    "net = models.vgg16(pretrained=use_pretrained)\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change Last Layer\n",
    "# Output Features 1000 → 2\n",
    "net.classifier[6] = nn.Linear(in_features=4096, out_features=2)\n",
    "print('Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify The Layers for updating\n",
    "params_to_update = []\n",
    "\n",
    "update_params_name = ['classifier.6.weight', 'classifier.6.bias']\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if name in update_params_name:\n",
    "        param.requires_grad = True\n",
    "        params_to_update.append(param)\n",
    "        print(name)\n",
    "    else:\n",
    "        param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(net, dataloader_dict, criterion, optimizer, num_epoch):\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epoch))\n",
    "        print('-'*20)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloader_dict[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    return net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train\n",
    "num_epoch = 2\n",
    "net = train_model(net, dataloader_dict, criterion, optimizer, num_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prediction\n",
    "id_list = []\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_path in tqdm(test_list):\n",
    "        img = Image.open(test_path)\n",
    "        _id = int(test_path.split('/')[-1].split('.')[0])\n",
    "\n",
    "        transform = ImageTransform(size, mean, std)\n",
    "        img = transform(img, phase='val')\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        outputs = net(img)\n",
    "        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n",
    "        \n",
    "        id_list.append(_id)\n",
    "        pred_list.append(preds[0])\n",
    "    \n",
    "    \n",
    "res = pd.DataFrame({\n",
    "    'id': id_list,\n",
    "    'label': pred_list\n",
    "})\n",
    "\n",
    "res.sort_values(by='id', inplace=True)\n",
    "res.reset_index(drop=True, inplace=True)\n",
    "\n",
    "res.to_csv('submission_vgg16_reduced.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize Prediction\n",
    "id_list = []\n",
    "class_ = {0: 'cat', 1: 'dog'}\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 12), facecolor='w')\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    \n",
    "    i = random.choice(res['id'].values)\n",
    "    \n",
    "    label = res.loc[res['id'] == i, 'label'].values[0]\n",
    "    if label > 0.5:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    img_path = os.path.join(test_dir, '{}.jpg'.format(i))\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    ax.set_title(class_[label])\n",
    "    ax.imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(net.state_dict(), \"./dogs-vs-cats-reduced/model_VGG16.pth\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 課題２"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "import numpy as np\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set()\n",
    "dataset = datasets.load_iris()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "y_name = dataset.target_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"品種名: \", y_name)\n",
    "# print(\"目的変数: \", y)\n",
    "# print(\"説明変数: \", X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# データ２分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "まずは普通にロジスティック回帰してみる"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ロジスティック回帰\n",
    "clf = linear_model.LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習結果\n",
    "a = clf.coef_\n",
    "b = clf.intercept_\n",
    "\n",
    "# print(\"回帰係数: \", a)\n",
    "# print(\"切片: \", b)\n",
    "\n",
    "# その予測精度\n",
    "print(\"正則化なしの訓練データでの点数: \", clf.score(X_train,y_train))\n",
    "print(\"正則化なしのテストデータでの点数: \", clf.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L2正則化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ロジスティック回帰 + L2正則化\n",
    "clf = linear_model.LogisticRegression(random_state=0, penalty='l2', n_jobs=1, C=1.0)\n",
    "clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習結果\n",
    "a = clf.coef_\n",
    "b = clf.intercept_\n",
    "\n",
    "print(\"回帰係数: \", a)\n",
    "print(\"切片: \", b)\n",
    "\n",
    "# その予測精度\n",
    "print(\"L2正則化の訓練データでの点数: \", clf.score(X_train,y_train))\n",
    "print(\"L2正則のテストデータでの点数: \", clf.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L1正則化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ロジスティック回帰 + L1正則化\n",
    "clf = linear_model.LogisticRegression(random_state=0, penalty='l1', n_jobs=1, C=1.0, solver='saga')\n",
    "clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習結果\n",
    "a = clf.coef_\n",
    "b = clf.intercept_\n",
    "\n",
    "# print(\"回帰係数: \", a)\n",
    "# print(\"切片: \", b)\n",
    "\n",
    "# その予測精度\n",
    "print(\"L2正則化の訓練データでの点数: \", clf.score(X_train,y_train))\n",
    "print(\"L2正則のテストデータでの点数: \", clf.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ランダムフォレストで特徴量の重要度可視化\n",
    "\n",
    "0=がく片の長さ，1=がく片の幅，2=花弁の長さ，3=花弁の幅 [cm]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 回帰木\n",
    "rf = ensemble.RandomForestRegressor()\n",
    "\n",
    "viz = FeatureImportances(rf)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LASSO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "viz = FeatureImportances(clf, relative=False)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 課題３"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "import os, sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats as st\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.covariance import GraphLasso\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import font_manager\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "データ取得"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set()\n",
    "dataset = datasets.load_iris()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "label = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "P = X.shape[1]\n",
    "y = dataset.target\n",
    "y_name = dataset.target_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img_path = \"img/\"\n",
    "if not os.path.exists(img_path):\n",
    "    os.mkdir(img_path)\n",
    "\n",
    "# parametars\n",
    "alpha = 0.2 # L１正則化パラメーター\n",
    "\n",
    "# Scikit LearnのGraphical Lassoを使ってブロック降下法で分散共分散行列、精度行列を求める\n",
    "model = GraphicalLasso(alpha=alpha,\n",
    "                     max_iter=100,                     \n",
    "                     verbose=True,\n",
    "                     assume_centered = True)\n",
    "\n",
    "model.fit(X)\n",
    "cov_ = model.covariance_ # 分散共分散行列\n",
    "prec_ = model.precision_ # 精度行列\n",
    "\n",
    "# Scikit LearnのGraphical Lassoの結果表示\n",
    "plt.figure(figsize=(10,4))\n",
    "ax = plt.subplot(121)\n",
    "sns.heatmap(cov_, annot=cov_, fmt='0.2f', ax=ax, xticklabels=label, yticklabels=label)\n",
    "plt.title(\"Graphical Lasso: Covariance matrix\")\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "sns.heatmap(prec_, annot=prec_, fmt='0.2f', ax=ax, xticklabels=label, yticklabels=label)\n",
    "plt.title(\"Graphical Lasso: Precision matrix\")\n",
    "plt.savefig(img_path+\"glasso_cov_prec.png\", dpi=128)\n",
    "plt.show()\n",
    "\n",
    "# 相関行列の算出\n",
    "cor = np.empty_like(cov_)\n",
    "for i in range(P):\n",
    "    for j in range(P):\n",
    "        cor[i, j] = cov_[i, j]/np.sqrt(cov_[i, i]*cov_[j, j])\n",
    "        \n",
    "# 偏相関行列の算出\n",
    "rho = np.empty_like(prec_)\n",
    "for i in range(P):\n",
    "    for j in range(P):\n",
    "        rho[i, j] = -prec_[i, j]/np.sqrt(prec_[i, i]*prec_[j, j])\n",
    "        \n",
    "plt.figure(figsize=(11,4))\n",
    "ax = plt.subplot(122)\n",
    "sns.heatmap(pd.DataFrame(rho), annot=rho, fmt='0.2f', ax=ax, xticklabels=label, yticklabels=label)\n",
    "plt.title(\"Partial correlation Coefficiant with Scikit-Learn\")\n",
    "#plt.savefig(img_path+\"partial_corr_sklearn.png\", dpi=128)\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "sns.heatmap(pd.DataFrame(cor), annot=cor, fmt='0.2f', ax=ax, xticklabels=label, yticklabels=label)\n",
    "plt.title(\"Correlation Coefficiant with Scikit-Learn\")\n",
    "plt.savefig(img_path+\"corr_pcorr_sklearn.png\", dpi=128)\n",
    "plt.show()\n",
    "\n",
    "# ちゃんと単位行列になっているか確認してみる。\n",
    "sns.heatmap(np.dot(cov_, prec_), annot=np.dot(cov_, prec_))\n",
    "plt.savefig(img_path+\"glasso_inv.png\", dpi=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 課題４"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score  \n",
    "from sklearn.metrics import recall_score   \n",
    "from sklearn.metrics import f1_score  \n",
    "from sklearn import metrics\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "import os\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats as st\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.covariance import GraphLasso\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import font_manager\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from collections import Counter\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import ClusterCentroids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8.6:1\n",
    "ecoli = fetch_datasets()['ecoli']\n",
    "# ecoli.data.shape\n",
    "X_ecoli = ecoli.data\n",
    "y_ecoli = ecoli.target\n",
    "print(sorted(Counter(y_ecoli).items()))\n",
    "\n",
    "# 15:1\n",
    "thyroid_sick = fetch_datasets()['thyroid_sick']\n",
    "X_thyroid_sick = thyroid_sick.data\n",
    "y_thyroid_sick = thyroid_sick.target\n",
    "print(sorted(Counter(y_thyroid_sick).items()))\n",
    "\n",
    "# 34:1\n",
    "ozone_level = fetch_datasets()['ozone_level']\n",
    "X_ozone_level = ozone_level.data\n",
    "y_ozone_level = ozone_level.target\n",
    "print(sorted(Counter(y_ozone_level).items()))\n",
    "\n",
    "# 130:1\n",
    "abalone_19 = fetch_datasets()['abalone_19']\n",
    "X_abalone_19 = abalone_19.data\n",
    "y_abalone_19 = abalone_19.target\n",
    "print(sorted(Counter(y_abalone_19).items()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下準備\n",
    "\n",
    "以下の関数は、アルゴリズムの特性を説明するために、\n",
    "リサンプリング後のサンプル空間をプロットするために使用されます。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_resampling(X, y, sampling, ax):\n",
    "    X_res, y_res = sampling.fit_resample(X, y)\n",
    "    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n",
    "    # make nice plotting\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    return Counter(y_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, ax):\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "分類してみる\n",
    "\n",
    "多次元だがx_0,x_1を取り出して２次元平面で表示"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "clf_ecoli = SVC(kernel='linear').fit(X_ecoli[:,:2], y_ecoli)\n",
    "plot_decision_function(X_ecoli[:,:2],y_ecoli,clf_ecoli,ax1)\n",
    "ax1.set_title('Linear SVC with y={}'.format(Counter(y_ecoli)))\n",
    "\n",
    "clf_thyroid_sick: object = SVC(kernel='linear').fit(X_thyroid_sick[:,:2], y_thyroid_sick)\n",
    "plot_decision_function(X_thyroid_sick[:,:2],y_thyroid_sick,clf_thyroid_sick,ax2)\n",
    "ax2.set_title('Linear SVC with y={}'.format(Counter(y_thyroid_sick)))\n",
    "\n",
    "clf_ozone_level = SVC(kernel='linear').fit(X_ozone_level[:,:2], y_ozone_level)\n",
    "plot_decision_function(X_ozone_level[:,:2],y_ozone_level,clf_ozone_level,ax3)\n",
    "ax3.set_title('Linear SVC with y={}'.format(Counter(y_ozone_level)))\n",
    "\n",
    "clf_abalone_19 = SVC(kernel='linear').fit(X_abalone_19[:,:2], y_abalone_19)\n",
    "plot_decision_function(X_abalone_19[:,:2],y_abalone_19,clf_abalone_19,ax4)\n",
    "ax4.set_title('Linear SVC with y={}'.format(Counter(y_abalone_19)))\n",
    "\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "精度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習\n",
    "clf_ecoli = SVC(kernel='linear',probability=True).fit(X_ecoli, y_ecoli)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = {}\n",
    "\n",
    "cmx_data = confusion_matrix(y_ecoli, clf_ecoli.predict(X_ecoli))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"ecoli\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_ecoli, X_ecoli, y_ecoli)\n",
    "plt.show()\n",
    "\n",
    "scores[(\"ecoli\",'accuracy')] = accuracy_score(y_ecoli, clf_ecoli.predict(X_ecoli))\n",
    "scores[(\"ecoli\",\"recall\")] = recall_score(y_ecoli, clf_ecoli.predict(X_ecoli), pos_label=1)\n",
    "scores[(\"ecoli\",\"precision\")] = precision_score(y_ecoli, clf_ecoli.predict(X_ecoli), pos_label=1)\n",
    "scores[(\"ecoli\",\"f1_score\")] = f1_score(y_ecoli, clf_ecoli.predict(X_ecoli), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習\n",
    "clf_thyroid_sick = SVC(kernel='linear', probability=True).fit(X_thyroid_sick, y_thyroid_sick)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_thyroid_sick, clf_thyroid_sick.predict(X_thyroid_sick))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"thyroid\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_thyroid_sick, X_thyroid_sick, y_thyroid_sick)\n",
    "plt.show()\n",
    "\n",
    "scores[(\"thyroid\",'accuracy')] = accuracy_score(y_thyroid_sick, clf_thyroid_sick.predict(X_thyroid_sick))\n",
    "scores[(\"thyroid\",\"recall\")] = recall_score(y_thyroid_sick, clf_thyroid_sick.predict(X_thyroid_sick), pos_label=1)\n",
    "scores[(\"thyroid\",\"precision\")] = precision_score(y_thyroid_sick, clf_thyroid_sick.predict(X_thyroid_sick), pos_label=1)\n",
    "scores[(\"thyroid\",\"f1_score\")] = f1_score(y_thyroid_sick, clf_thyroid_sick.predict(X_thyroid_sick), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習\n",
    "clf_ozone_level = SVC(kernel='linear',probability=True).fit(X_ozone_level, y_ozone_level)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_ozone_level, clf_ozone_level.predict(X_ozone_level))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"ozone_level\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_ozone_level, X_ozone_level, y_ozone_level)\n",
    "plt.show()\n",
    "\n",
    "scores[(\"ozone_level\",'accuracy')] = accuracy_score(y_ozone_level, clf_ozone_level.predict(X_ozone_level))\n",
    "scores[(\"ozone_level\",\"recall\")] = recall_score(y_ozone_level, clf_ozone_level.predict(X_ozone_level), pos_label=1)\n",
    "scores[(\"ozone_level\",\"precision\")] = precision_score(y_ozone_level, clf_ozone_level.predict(X_ozone_level), pos_label=1)\n",
    "scores[(\"ozone_level\",\"f1_score\")] = f1_score(y_ozone_level, clf_ozone_level.predict(X_ozone_level), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習\n",
    "clf_abalone_19 = SVC(kernel='linear',probability=True).fit(X_abalone_19, y_abalone_19)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_abalone_19, clf_abalone_19.predict(X_abalone_19))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"abalone_19\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_abalone_19, X_abalone_19, y_abalone_19)\n",
    "plt.show()\n",
    "\n",
    "scores[(\"abalone_19\",'accuracy')] = accuracy_score(y_abalone_19, clf_abalone_19.predict(X_abalone_19))\n",
    "scores[(\"abalone_19\",\"recall\")] = recall_score(y_abalone_19, clf_abalone_19.predict(X_abalone_19), pos_label=1)\n",
    "scores[(\"abalone_19\",\"precision\")] = precision_score(y_abalone_19, clf_abalone_19.predict(X_abalone_19), pos_label=1)\n",
    "scores[(\"abalone_19\",\"f1_score\")] = f1_score(y_abalone_19, clf_abalone_19.predict(X_abalone_19), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "精度(Accuracy)：真陽性と真陰性の割合の大きさ\n",
    "\n",
    "適合率(Precision)：正と予測したもののうち実際に正だった割合\n",
    "\n",
    "再現率(Recall)：正だったもののうち正と予測されていたものの割合\n",
    "\n",
    "F値：再現率と適合率の調和平均"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Metrics:\")\n",
    "print(pd.Series(scores).unstack())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "オーバーサンプリング：トレーニングデータの少ないクラスを疑似的に増やす"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_oversampled = {}\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_ecoli,y_ecoli)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"ecoli oversampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_oversampled[(\"ecoli\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_oversampled[(\"ecoli\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"ecoli\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"ecoli\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax1= plt.subplots(1, 1, figsize=(3, 3))\n",
    "\n",
    "clf_resampled = SVC(kernel='linear').fit(X_resampled[:,:2], y_resampled)\n",
    "plot_decision_function(X_resampled[:,:2],y_resampled,clf_resampled,ax1)\n",
    "ax1.set_title('Linear SVC with y={}'.format(Counter(y_resampled)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = ros.fit_resample(X_thyroid_sick,y_thyroid_sick)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"thyroid_sick oversampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_oversampled[(\"thyroid_sick\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_oversampled[(\"thyroid_sick\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"thyroid_sick\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"thyroid_sick\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = ros.fit_resample(X_ozone_level,y_ozone_level)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"ozone_level oversampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_oversampled[(\"ozone_level\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_oversampled[(\"ozone_level\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"ozone_level\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"ozone_level\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = ros.fit_resample(X_abalone_19,y_abalone_19)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"abalone_19 oversampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_oversampled[(\"abalone_19\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_oversampled[(\"abalone_19\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"abalone_19\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_oversampled[(\"abalone_19\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下にオーバーサンプル前後の指標を比較する\n",
    "\n",
    "精度は落ちるものの他の指標が非常に改善されている．"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Previous Metrics:\")\n",
    "print(pd.Series(scores).unstack())\n",
    "\n",
    "print(\"Oversampled Metrics\")\n",
    "print(pd.Series(scores_oversampled).unstack())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "アンダーサンプリング：データの多い方を擬似的に減らす"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_undersampled = {}\n",
    "\n",
    "cc = ClusterCentroids(random_state=0)\n",
    "X_resampled, y_resampled = cc.fit_resample(X_ecoli,y_ecoli)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"ecoli undersampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_undersampled[(\"ecoli\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_undersampled[(\"ecoli\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"ecoli\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"ecoli\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = cc.fit_resample(X_thyroid_sick,y_thyroid_sick)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"thyroid_sick undersampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_undersampled[(\"thyroid_sick\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_undersampled[(\"thyroid_sick\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"thyroid_sick\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"thyroid_sick\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = cc.fit_resample(X_ozone_level,y_ozone_level)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"ozone_level undersampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_undersampled[(\"ozone_level\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_undersampled[(\"ozone_level\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"ozone_level\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"ozone_level\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = cc.fit_resample(X_abalone_19,y_abalone_19)\n",
    "\n",
    "clf_resampled = SVC(kernel='linear',probability=True).fit(X_resampled, y_resampled)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cmx_data = confusion_matrix(y_resampled, clf_resampled.predict(X_resampled))\n",
    "df_cmx = pd.DataFrame(cmx_data)\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(df_cmx, fmt='d', annot=True, square=True)\n",
    "plt.title(\"abalone_19 undersampled\")\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n",
    "\n",
    "metrics.plot_roc_curve(clf_resampled, X_resampled, y_resampled)\n",
    "plt.show()\n",
    "\n",
    "scores_undersampled[(\"abalone_19\",'accuracy')] = accuracy_score(y_resampled, clf_resampled.predict(X_resampled))\n",
    "scores_undersampled[(\"abalone_19\",\"recall\")] = recall_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"abalone_19\",\"precision\")] = precision_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)\n",
    "scores_undersampled[(\"abalone_19\",\"f1_score\")] = f1_score(y_resampled, clf_resampled.predict(X_resampled), pos_label=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下にサンプル加工前後の指標を比較する"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Previous Metrics:\")\n",
    "print(pd.Series(scores).unstack())\n",
    "\n",
    "print(\"Oversampled Metrics\")\n",
    "print(pd.Series(scores_oversampled).unstack())\n",
    "\n",
    "print(\"Undersampled Metrics\")\n",
    "print(pd.Series(scores_undersampled).unstack())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "指標を見ると，ecoli や abalone_19 データセットではオーバーサンプリングが，\n",
    "ozone_level や thyroid_sick データセットではアンダーサンプリングが良い改善をしている．"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 課題５"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 課題５－１：特徴選択\n",
    "\n",
    "まず yellowbrick の 再帰特徴削減(RFE) を走らせてみる．"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "再帰的特徴除去（RFE）は、モデルを適合させ、\n",
    "指定された特徴数に達するまで最も弱い特徴（または特徴）を除去する特徴選択法です。\n",
    "特徴はモデルのcoef_またはfeature_importances_属性によってランク付けされ、\n",
    "ループごとに少数の特徴を再帰的に除去することで、\n",
    "RFEはモデルに存在する可能性のある依存性と共線性を除去しようとします。\n",
    "\n",
    "RFEでは、保持するために指定された数の特徴を必要としますが、\n",
    "どれだけの特徴が有効であるかは事前にわからないことが多いです。\n",
    "最適な特徴の数を見つけるために、RFEでクロスバリデーションを使用して、\n",
    "異なる特徴のサブセットをスコア化し、最もスコア化された特徴の集合を選択します。\n",
    "RFECV ビジュアライザーは、モデル内の特徴の数をクロスバリデーションされたテスト・スコアと\n",
    "ばらつきとともにプロットし、選択された特徴の数を可視化します。\n",
    "\n",
    "これが実際にどのように機能するかを示すために、\n",
    "25個のうち情報量の多い特徴を3個しか持たないデータセットを使った例から始めます。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from yellowbrick.model_selection import RFECV\n",
    "\n",
    "# Create a dataset with only 3 informative features\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=25, n_informative=3, n_redundant=2,\n",
    "    n_repeated=0, n_classes=8, n_clusters_per_class=1, random_state=0\n",
    ")\n",
    "\n",
    "# Instantiate RFECV visualizer with a linear SVM classifier\n",
    "visualizer = RFECV(SVC(kernel='linear', C=1))\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "この図は理想的なRFECV曲線を示しており、3つの情報的特徴が捕捉されると曲線は優れた精度にジャンプし、\n",
    "その後、非情報的特徴がモデルに追加されると徐々に精度が低下します。\n",
    "網掛けの領域は、クロスバリデーションの変動性を表し、\n",
    "曲線によって描かれた平均精度スコアの上と下の1標準偏差を表しています。\n",
    "\n",
    "実際のデータセットを探索すると、信用デフォルトのバイナリ分類器でのRFECVの影響を見ることができます。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from yellowbrick.datasets import load_credit\n",
    "\n",
    "# Load classification dataset\n",
    "X, y = load_credit()\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "visualizer = RFECV(RandomForestClassifier(), cv=cv, scoring='f1_weighted')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "この例では、19個の特徴が選択されていることがわかりますが、\n",
    "5個程度の特徴の後ではモデルのf1スコアはあまり改善されていないように見えます。\n",
    "除去する特徴の選択は、各再帰の結果を決定する上で大きな役割を果たします。\n",
    "各ステップで複数の特徴を除去するようにステップパラメータを変更すると、\n",
    "最悪の特徴を早期に除去し、残りの特徴を強化するのに役立つかもしれません\n",
    "（また、特徴の数が多いデータセットの特徴除去を高速化するために使用することもできます）。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UCIのデータセットなど選んで再帰特徴削減(RFE) をやってみる\n",
    "\n",
    "yellowbrick のデータセットにある occupancy データセットを取り出して実行"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_occupancy\n",
    "\n",
    "X, y = load_occupancy()\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "visualizer = RFECV(RandomForestClassifier(), cv=cv, scoring='f1_weighted')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 課題５－２：検証曲線・学習曲線\n",
    "\n",
    "まず yellowbrick の ’learning curve’ での例を走らせる．"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "学習曲線は、訓練サンプル数が変化する推定量について、訓練スコアと交差検証テストスコアの関係を示しています。\n",
    "この可視化は通常、次の2つのことを示すために使用されます。\n",
    "\n",
    "1. 推定量が増えることで推定量がどの程度恩恵を受けるか\n",
    "（例：「十分なデータがある」か、オンラインで使用すると推定量が向上するかなど）。\n",
    "1. 推定器が分散に起因する誤差とバイアスに起因する誤差のどちらに対してより敏感であるか。\n",
    "\n",
    "以下の学習曲線を考えてみましょう\n",
    "（Yellowbrickで生成されたものですが、scikit-learnドキュメントのPlotting Learning Curvesから生成されたものです）。\n",
    "\n",
    "![](https://www.scikit-yb.org/en/latest/_images/learning_curve_sklearn_example.png)\n",
    "\n",
    "より多くのデータが追加されるにつれて、学習スコアと交差検証スコアが一緒に収束する場合（左図に示す）、\n",
    "モデルはおそらくより多くのデータの恩恵を受けないでしょう。\n",
    "トレーニング・スコアが検証スコアよりもはるかに大きい場合、モデルはより効果的に一般化するために、\n",
    "おそらくより多くのトレーニング例を必要とします。\n",
    "\n",
    "曲線は平均スコアでプロットされていますが、クロス・バリデーション中の変動は、\n",
    "すべてのクロス・バリデーションの平均の上と下の標準偏差を表す網掛けの領域で示されています。\n",
    "モデルがバイアスのためにエラーを起こす場合、学習スコア曲線の周りには、おそらくより多くの変動があるでしょう。\n",
    "モデルが分散のためにエラーを起こす場合、交差検証されたスコアの周囲には、より多くのばらつきがあるでしょう。\n",
    "\n",
    "（注）学習曲線は、fit()メソッドとpredict()メソッド、および単一のスコアリングメトリックを持つすべての推定量について生成できます。\n",
    "これには、以下の例で見るように、分類器、回帰器、およびクラスタリングが含まれます。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification\n",
    "\n",
    "次の例では、分類モデルの学習曲線を可視化する方法を示します。\n",
    "DataFrameをロードし、カテゴリエンコーディングを実行した後、\n",
    "各分割内のすべてのクラスが同じ比率で表現されることを保証するために、StratifiedKFoldクロスバリデーション戦略を作成します。\n",
    "次に、分類器の精度とリコールの関係をよりよく理解するために、デフォルトのメトリックである精度ではなく、\n",
    "f1_加重スコアリングメトリックを使用してビジュアライザーを適合させます。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from yellowbrick.datasets import load_game\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Load a classification dataset\n",
    "X, y = load_game()\n",
    "\n",
    "# Encode the categorical data\n",
    "X = OneHotEncoder().fit_transform(X)\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Create the learning curve visualizer\n",
    "cv = StratifiedKFold(n_splits=12)\n",
    "sizes = np.linspace(0.3, 1.0, 10)\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "model = MultinomialNB()\n",
    "visualizer = LearningCurve(\n",
    "    model, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4\n",
    ")\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "この学習曲線は、高いテスト変動性と約30,000インスタンスまでの低いスコアを示しますが、このレベルを超えると、\n",
    "モデルは約0.6のF1スコアに収束し始めます。\n",
    "訓練とテストのスコアがまだ収束していないことがわかりますので、\n",
    "このモデルは潜在的にはより多くの訓練データの恩恵を受けるでしょう。\n",
    "\n",
    "最後に、このモデルは主に分散（テスト・データのCVスコアはトレーニング・データよりも変動が大きい）による誤差に苦しんでいるので、モデルがオーバーフィットしている可能性があります。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regression\n",
    "\n",
    "回帰の学習曲線を構築することは、直線的で非常に似ています。\n",
    "下の例では、データをロードしてターゲットを選択した後、決定係数またはR2スコアに従って学習曲線スコアを探索します。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from yellowbrick.datasets import load_energy\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Load a regression dataset\n",
    "X, y = load_energy()\n",
    "\n",
    "# Instantiate the regression model and visualizer\n",
    "model = RidgeCV()\n",
    "visualizer = LearningCurve(model, scoring='r2')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "この学習曲線は非常に高い変動性を示し、約350個のインスタンスまではかなり低いスコアを示しています。\n",
    "非常に高いスコアで収束しているので、このモデルがより多くのデータの恩恵を受けられることは明らかです。\n",
    "潜在的に、より多くのデータと正則化のためのより大きなアルファがあれば、\n",
    "このモデルはテスト・データでの変動がはるかに少なくなるでしょう。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clustering\n",
    "\n",
    "学習曲線はクラスタリングモデルにも機能し、\n",
    "シルエットスコアや密度スコアのようなクラスタの形状や組織を指定するメトリクスを使用することができます。\n",
    "メンバーシップが事前に分かっている場合は、\n",
    "以下のようにランドスコアを使用してクラスタリング性能を比較することができます。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Generate synthetic dataset with 5 random clusters\n",
    "X, y = make_blobs(n_samples=1000, centers=5, random_state=42)\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = LearningCurve(model, scoring=\"adjusted_rand_score\", random_state=42)\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "残念ながら、ランダムデータでは、これらの曲線は非常に変動しますが、クラスタリングに特有の項目を指摘するのに役立ちます。\n",
    "第一に、y軸が非常に狭いことに注意してください。\n",
    "大まかに言えば、これらの曲線は収束しており、実際にはクラスタリングアルゴリズムが非常にうまく機能しています。\n",
    "第二に、クラスタリングにおいて、データポイントの収束は必ずしも悪いことではありません；\n",
    "実際、我々はより多くのデータが追加されても、トレーニングスコアとクロスバリデーションスコアが発散しないようにしたいと考えています。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quick Method\n",
    "\n",
    "同じ機能は、関連するクイックメソッド `learning_curve` で実現できます。\n",
    "このメソッドは、関連付けられた引数で `LearningCurve` オブジェクトを構築し、\n",
    "それをフィットさせてから (オプションで) すぐに可視化を表示します。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from yellowbrick.datasets import load_energy\n",
    "from yellowbrick.model_selection import learning_curve\n",
    "\n",
    "# Load a regression dataset\n",
    "X, y = load_energy()\n",
    "\n",
    "learning_curve(RidgeCV(), X, y, scoring='r2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UCIの他のデータセットから選んで学習曲線を見てみる\n",
    "\n",
    "Concrete を使用して Regression で学習曲線を見る"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from yellowbrick.datasets import load_concrete\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Load a regression dataset\n",
    "X, y = load_concrete()\n",
    "\n",
    "# Instantiate the regression model and visualizer\n",
    "model = RidgeCV()\n",
    "visualizer = LearningCurve(model, scoring='r2')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "energy と同様にうまく学習がすすんでいる．"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 課題６"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Pythonで学ぶ統計的機械学習 - 密度推定](https://github.com/kanamori-takafumi/book_StatMachineLearn_with_Python/blob/master/ch14densityratio.ipynb) を試してみる\n",
    "\n",
    "密度比の推定"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# データ設定 \n",
    "n, m = 100, 200\n",
    "nu_mean, nu_sd = -0.5, 1\n",
    "a_mean,  a_sd = 1, 0.8\n",
    "newdat = np.linspace(-4,4,500).reshape(500,1)  # 予測点\n",
    "tnu =  norm.pdf(newdat, nu_mean, nu_sd)            # 確率密度の計算\n",
    "tde = (norm.pdf(newdat, a_mean, a_sd)+tnu)/2\n",
    "tw = tnu/tde                                   # 予測点上での真の密度比"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# データ生成\n",
    "nu = np.random.normal(loc=nu_mean, scale=nu_sd, size=n).reshape(n,1)\n",
    "ma = np.random.binomial(m,0.5); mb = m-ma\n",
    "de = np.r_[np.random.normal(loc=nu_mean,scale=nu_sd,size=ma).reshape(ma,1), np.random.normal(loc=a_mean, scale=a_sd, size=mb).reshape(mb,1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class kernelDensityRatio:\n",
    "    \"\"\"\n",
    "    kernel density-ratio estimator using Gaussian kernel\n",
    "    gamma: bandwidth of Gaussian kernel\n",
    "    lam: regularizaiton parameter\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=None, lam=None):\n",
    "        self.gamma = gamma             # カーネル幅\n",
    "        self.lam = lam                 # 正則化パラメータ\n",
    "        \n",
    "    def fit(self, de, nu):             # 密度比推定\n",
    "        if self.gamma is None:\n",
    "            ma = nu.shape[0] + de.shape[0]\n",
    "            idx = np.random.choice(ma,round(ma/2))\n",
    "            self.gamma = (1/np.median(distance.pdist(np.r_[nu,de][idx,:])))**2\n",
    "        if self.lam is None:\n",
    "            self.lam = (min(nu.shape[0], de.shape[0]))**(-0.9)\n",
    "        gamma = self.gamma; lam = self.lam\n",
    "        n = de.shape[0]\n",
    "        # グラム行列の計算\n",
    "        Kdd = rbf_kernel(de, gamma=gamma)\n",
    "        Kdn = rbf_kernel(de, nu, gamma=gamma)\n",
    "        # 係数の推定\n",
    "        Amat = Kdd + n*lam*np.identity(n)\n",
    "        bvec = -np.mean(Kdn,1)/lam\n",
    "        self.alpha = np.linalg.solve(Amat, bvec)\n",
    "        self.de, self.nu = de, nu\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):               # 予測点 x での密度比の値\n",
    "        Wde =  np.dot(rbf_kernel(x, self.de, gamma=self.gamma), self.alpha)\n",
    "        Wnu = np.mean(rbf_kernel(x, self.nu, gamma=self.gamma),1)/self.lam\n",
    "        return np.maximum(Wde + Wnu,0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(newdat,tw,lw=2)            # 真の密度比関数のプロット\n",
    "# データ点のプロット\n",
    "plt.scatter(nu.reshape(n,),np.repeat(0.3,n),marker='.',c='black',s=20)\n",
    "plt.scatter(de.reshape(m,),np.repeat(0.1,m),marker='x',c='gray',s=20)\n",
    "# 以下のカーネル幅で推定\n",
    "gammas = np.array([0.01, 0.1, 1])\n",
    "for g in gammas:\n",
    "    dr = kernelDensityRatio(gamma=g)\n",
    "    dr.fit(de,nu)                   # データへのフィッティング\n",
    "    drp = dr.predict(newdat)        # 密度比の予測値\n",
    "    plt.plot(newdat,drp, c='red',linestyle='dashed',lw=2)  # プロット\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "密度比推定のための交差検証法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.spatial import distance        # distanceの計算\n",
    "cvk = 5                                                      # 交差検証法のK\n",
    "n, m = nu.shape[0], de.shape[0]          # データ数\n",
    "# カーネル幅パラメータの候補を生成\n",
    "idx = np.random.choice(n+m,round((n+m)/2))\n",
    "gammas = 1/np.percentile(distance.pdist(np.r_[nu,de][idx,:]),[1,99])**2\n",
    "gammas = np.logspace(np.log10(gammas.min()/100), np.log10(gammas.max()*100),10)\n",
    "# 正則化パラメータ lambda の候補を生成 \n",
    "lams = np.array([(min(n,m))**(-0.9)])\n",
    "# モデルパラメータの候補\n",
    "modelpars = np.array([(x,y) for x in gammas for y in lams])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# それぞれのデータを5グループに分ける\n",
    "inu = np.repeat(np.arange(cvk),np.ceil(n/cvk))\n",
    "inu = inu[np.random.choice(n,n,replace=False)]\n",
    "ide = np.repeat(np.arange(cvk),np.ceil(m/cvk))\n",
    "ide = ide[np.random.choice(m,m,replace=False)]\n",
    "cvloss = []\n",
    "for gamma, lam in modelpars:\n",
    "    tcvloss = []\n",
    "    for k in np.arange(cvk):\n",
    "        # トレーニングデータ\n",
    "        trnu, trde = nu[inu!=k,:], de[ide!=k,:]\n",
    "        # テストデータ\n",
    "        tenu, tede = nu[inu==k,:], de[ide==k,:]\n",
    "        # 指定されたモデルパラメータで密度比を推定\n",
    "        kdr =kernelDensityRatio(gamma=gamma, lam=lam)\n",
    "        kdr.fit(trde, trnu)                             # 推定\n",
    "        wde = kdr.predict(tede)                         # de上で予測\n",
    "        wnu = kdr.predict(tenu)                         # nu上で予測\n",
    "        tcvloss.append(np.mean(wde**2)/2-np.mean(wnu))  # 二乗損失\n",
    "    cvloss.append(np.mean(tcvloss))\n",
    "# 最適なモデルパラメータ\n",
    "optgamma,optlam = modelpars[np.argmin(cvloss),:]\n",
    "optgamma,optlam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.xscale('log'); plt.xlabel('gamma'); plt.ylabel('cv loss')\n",
    "plt.scatter(gammas, cvloss)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 密度比の推定\n",
    "kdr =kernelDensityRatio(gamma=optgamma, lam=optlam)\n",
    "kdr.fit(de,nu)\n",
    "kdr.predict(newdat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "共変量シフトの下での回帰分析"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 真の回帰関数 \n",
    "def f(x): \n",
    "    return (x+2)*(x-3)*x\n",
    "# データ設定\n",
    "ntr, mtr, sdtr   = 100, -1.4, 0.7\n",
    "nte, mte, sdte = 100,  0.8, 0.8\n",
    "# トレーニングデータ生成\n",
    "xtr = np.random.normal(loc=mtr, scale=sdtr, size=ntr).reshape(ntr,1)\n",
    "ytr = f(xtr) + np.random.normal(scale=2, size=ntr).reshape(ntr,1)\n",
    "# テストデータ生成\n",
    "xte = np.random.normal(loc=mte, scale=sdte, size=nte).reshape(nte,1)\n",
    "yte = f(xte) + np.random.normal(scale=2, size=nte).reshape(nte,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# トレーニングデータ点上での密度比を推定\n",
    "kdr = kernelDensityRatio()\n",
    "kdr.fit(xtr,xte)\n",
    "pw = kdr.predict(xtr)\n",
    "# 重み付き最小二乗法で回帰パラメータを推定\n",
    "W = np.sqrt(np.diag(pw))\n",
    "X =  sm.add_constant(xtr)\n",
    "WX = np.dot(W,X); WY = np.dot(W, ytr)\n",
    "estTheta = np.linalg.solve(np.dot(WX.T,WX), np.dot(WX.T,WY))\n",
    "estTheta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二標本検定"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn import datasets\n",
    "d = datasets.load_breast_cancer()        # データ読込み\n",
    "de = d.data[d.target==0]; de = scale(de) # スケーリング\n",
    "nu = d.data[d.target==1]; nu = scale(nu) # スケーリング"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kdr =kernelDensityRatio() \n",
    "kdr.fit(de,nu)                               # 密度比の推定\n",
    "L1distEst = np.mean(abs(1-kdr.predict(de)))  # L1距離の推定値\n",
    "nperm = 10000                                # 並べ替え検定の繰り返し数\n",
    "nde, nnu = de.shape[0], nu.shape[0]\n",
    "dall = np.r_[de,nu]\n",
    "permL1dist = []\n",
    "for itr in np.arange(nperm):\n",
    "    idx = np.random.choice(nde+nnu,nde,replace=False)   # データの並べ替え\n",
    "    perm_de = dall[idx,:]\n",
    "    perm_nu = np.delete(dall,idx,0)\n",
    "    pdr =kernelDensityRatio()\n",
    "    pdr.fit(perm_de, perm_nu)            # 並べ替えデータに対する密度比の推定\n",
    "    permL1dist.append(np.mean(abs(1-pdr.predict(perm_de)))) # L1距離の推定\n",
    "np.mean(L1distEst < np.array(permL1dist))          # 並べ替え検定による p値"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`banana` データセットでの共変量シフト問題"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, glob, time, copy, random, zipfile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.listdir('./data/banana')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train_dir, Test_dir\n",
    "base_dir = './data/banana'\n",
    "train_dir = base_dir + '/train'\n",
    "test_dir  = base_dir + '/test'\n",
    "\n",
    "# Check File Name\n",
    "os.listdir(train_dir)[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FilePath List\n",
    "train_data_list = glob.glob(os.path.join(train_dir , 'banana_train_data_*.asc'))\n",
    "train_label_list = glob.glob(os.path.join(train_dir , 'banana_train_labels_*.asc'))\n",
    "test_data_list = glob.glob(os.path.join(test_dir, '*banana_test_data_*.asc'))\n",
    "test_label_list = glob.glob(os.path.join(test_dir, '*banana_test_labels_*.asc'))\n",
    "\n",
    "train_data_list = [string.replace('\\\\', '/') for string in train_data_list]\n",
    "train_label_list = [string.replace('\\\\', '/') for string in train_label_list]\n",
    "test_data_list = [string.replace('\\\\', '/') for string in test_data_list]\n",
    "test_label_list = [string.replace('\\\\', '/') for string in test_label_list]\n",
    "\n",
    "print(train_data_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習データ読み込み\n",
    "f = open(train_data_list[0])\n",
    "X_train = [s.lstrip() for s in f.read().split('\\n')]\n",
    "X_train = [ list(map(float, list(filter(lambda a: a != '', s.split(' '))))) for s in X_train ]\n",
    "X_train = [ x for x in X_train if len(x) == 2 ]\n",
    "print([i for i,x in enumerate(X_train) if len(x) != 2])\n",
    "X_train = np.array(X_train, dtype=float)\n",
    "print(X_train.shape)\n",
    "print(X_train.dtype)\n",
    "f.close()\n",
    "\n",
    "f = open(train_label_list[0])\n",
    "y_train = [ s.lstrip() for s in f.read().split('\\n') ]\n",
    "y_train = [ list(map(float, list(filter(lambda a: a != '', s.split(' '))))) for s in y_train ]\n",
    "y_train = [ x for x in y_train if len(x) == 1 ]\n",
    "print([i for i,x in enumerate(y_train) if len(x) != 1])\n",
    "y_train = np.array(y_train, dtype=float)\n",
    "print(y_train.shape)\n",
    "print(y_train.dtype)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# テストデータ\n",
    "f = open(test_data_list[0])\n",
    "X_test = [s.lstrip() for s in f.read().split('\\n')]\n",
    "X_test = [ list(map(float, list(filter(lambda a: a != '', s.split(' '))))) for s in X_test ]\n",
    "X_test = [ x for x in X_test if len(x) == 2 ]\n",
    "print([i for i,x in enumerate(X_test) if len(x) != 2])\n",
    "X_test = np.array(X_test, dtype=float)\n",
    "print(X_test.shape)\n",
    "print(X_test.dtype)\n",
    "f.close()\n",
    "\n",
    "f = open(test_label_list[0])\n",
    "y_test = [ s.lstrip() for s in f.read().split('\\n') ]\n",
    "y_test = [ list(map(float, list(filter(lambda a: a != '', s.split(' '))))) for s in y_test ]\n",
    "y_test = [ x for x in y_test if len(x) == 1 ]\n",
    "print([i for i,x in enumerate(y_test) if len(x) != 1])\n",
    "y_test = np.array(y_test, dtype=float)\n",
    "print(y_test.shape)\n",
    "print(y_test.dtype)\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:,0], X_test[:,1])\n",
    "plt.scatter(X_train[:,0],X_train[:,1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # トレーニングデータ点上での密度比を推定\n",
    "# kdr = kernelDensityRatio()\n",
    "# kdr.fit(X_train, y_train)\n",
    "# pw = kdr.predict(X_train)\n",
    "# # 重み付き最小二乗法で回帰パラメータを推定\n",
    "# W = np.sqrt(np.diag(pw))\n",
    "# X =  sm.add_constant(X_train)\n",
    "# WX = np.dot(W,X); WY = np.dot(W, y_train)\n",
    "# estTheta = np.linalg.solve(np.dot(WX.T,WX), np.dot(WX.T,WY))\n",
    "# estTheta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習データのカーネル密度推定\n",
    "x = X_train[:,0]; y = X_train[:,1]\n",
    "df = pd.DataFrame(data=X_train, columns=['x1', 'x2'], dtype='float')\n",
    "g = sns.jointplot(x=\"x1\",y=\"x2\",data=df, kind=\"kde\")\n",
    "g.plot_joint(plt.scatter, c=\"c\", s=30, linewidth=1, marker=\"+\")\n",
    "g.ax_joint.collections[0].set_alpha(0)\n",
    "g.set_axis_labels(\"$X_1$\", \"$X_2$\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# テストデータのカーネル密度推定\n",
    "df = pd.DataFrame(data=X_test, columns=['x1', 'x2'], dtype='float')\n",
    "g = sns.jointplot(x=\"x1\",y=\"x2\",data=df, kind=\"kde\")\n",
    "g.plot_joint(plt.scatter, c=\"c\", s=30, linewidth=1, marker=\"+\")\n",
    "g.ax_joint.collections[0].set_alpha(0)\n",
    "g.set_axis_labels(\"$X_1$\", \"$X_2$\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}